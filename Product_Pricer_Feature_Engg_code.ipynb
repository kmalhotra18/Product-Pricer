{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eb45e7b-f84c-4d1c-ad34-56e564b69f40",
   "metadata": {},
   "source": [
    "# The Product Pricer Continued\n",
    "\n",
    "A model that can estimate how much something costs, from its description.\n",
    "\n",
    "## Baseline Models\n",
    "\n",
    "Working on the simplest models to act as a starting point that we will beat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3534774c-79c7-49df-8f73-e1e6d3f73edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7665e69-0e11-435b-b026-54b75346049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More imports for our traditional machine learning\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556a3b4f-eda5-42a9-ace6-23a872569839",
   "metadata": {},
   "source": [
    "## NLP imports\n",
    "\n",
    "More imports for our NLP related machine learning.  \n",
    "If the gensim import gives you an error like \"Cannot import name 'triu' from 'scipy.linalg' then run in another cell:  \n",
    "`!pip install \"scipy<1.13\"`  \n",
    "\n",
    "As described on StackOverflow [here](https://stackoverflow.com/questions/78279136/importerror-cannot-import-name-triu-from-scipy-linalg-when-importing-gens).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef57b717-18fd-498e-9c37-4c5b860dd15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"scipy<1.13\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dddf756c-099a-4c4e-870a-7df0dd92e4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!C:\\Users\\kunal\\AppData\\Local\\Programs\\Python\\Python313\\python.exe -m pip install gensim --prefer-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c064f1e4-fceb-4bfe-9a08-e962d75f7e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP related imports\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94da2193-548f-4f55-a806-ce9900f2b5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, more imports for more advanced machine learning\n",
    "\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "643df907-a12b-423c-b730-d9ea4f353a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants - used for printing to stdout in color\n",
    "\n",
    "GREEN = \"\\033[92m\"\n",
    "YELLOW = \"\\033[93m\"\n",
    "RED = \"\\033[91m\"\n",
    "RESET = \"\\033[0m\"\n",
    "COLOR_MAP = {\"red\":RED, \"orange\": YELLOW, \"green\": GREEN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "372ae844-26ef-434a-9446-45f089ffdae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment\n",
    "\n",
    "load_dotenv(override=True)\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')\n",
    "os.environ['ANTHROPIC_API_KEY'] = os.getenv('ANTHROPIC_API_KEY', 'your-key-if-not-using-env')\n",
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN', 'your-key-if-not-using-env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2a00657-31d2-4acf-802c-8fb88f0a17f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log in to HuggingFace\n",
    "\n",
    "hf_token = os.environ['HF_TOKEN']\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61276ad8-fdc0-4e87-bbca-9f31e9bb8eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One more import after logging in\n",
    "\n",
    "from items import Item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41af1e35-9949-4e2d-81e3-1ac15e004cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405c25a5-30d5-4690-b659-da3a774c9e90",
   "metadata": {},
   "source": [
    "# Loading the pkl files\n",
    "\n",
    "Let's avoid curating all our data again! Load in the pickle files previously created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd383873-14d3-42de-a865-2f7ea791adc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.pkl', 'rb') as file:\n",
    "    train = pickle.load(file)\n",
    "\n",
    "with open('test.pkl', 'rb') as file:\n",
    "    test = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43e677ee-d3af-4360-b3df-d04c4860992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remind ourselves the training prompt\n",
    "\n",
    "print(train[0].prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5023421-a212-467c-886d-0d637e500e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remind a test prompt\n",
    "\n",
    "print(test[0].test_prompt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac49aaf2-b13a-4a98-8e53-77258fc0e2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remind a actual price for test prompt\n",
    "\n",
    "print(train[0].price)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dcd35b-f4bb-4b2e-97b4-957e223e59c0",
   "metadata": {},
   "source": [
    "## Unveiling a mighty script that we will use a lot!\n",
    "\n",
    "A rather pleasing Test Harness that will evaluate any model against 250 items from the Test set\n",
    "\n",
    "And show us the results in a visually satisfying way.\n",
    "\n",
    "You write a function of this form:\n",
    "\n",
    "```\n",
    "def my_prediction_function(item):\n",
    "    # my code here\n",
    "    return my_estimate\n",
    "```\n",
    "\n",
    "And then you call:\n",
    "\n",
    "`Tester.test(my_prediction_function)`\n",
    "\n",
    "To evaluate your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc548c8f-3837-4d00-9a39-4f28ba8842ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tester:\n",
    "\n",
    "    def __init__(self, predictor, title=None, data=test, size=250):\n",
    "        self.predictor = predictor\n",
    "        self.data = data\n",
    "        self.title = title or predictor.__name__.replace(\"_\", \" \").title()\n",
    "        self.size = size\n",
    "        self.guesses = []\n",
    "        self.truths = []\n",
    "        self.errors = []\n",
    "        self.sles = []\n",
    "        self.colors = []\n",
    "\n",
    "    def color_for(self, error, truth):\n",
    "        if error<40 or error/truth < 0.2:\n",
    "            return \"green\"\n",
    "        elif error<80 or error/truth < 0.4:\n",
    "            return \"orange\"\n",
    "        else:\n",
    "            return \"red\"\n",
    "    \n",
    "    def run_datapoint(self, i):\n",
    "        datapoint = self.data[i]\n",
    "        guess = self.predictor(datapoint)\n",
    "        truth = datapoint.price\n",
    "        error = abs(guess - truth)\n",
    "        log_error = math.log(truth+1) - math.log(guess+1)\n",
    "        sle = log_error ** 2  # Squared Logged Error\n",
    "        color = self.color_for(error, truth)\n",
    "        title = datapoint.title if len(datapoint.title) <= 40 else datapoint.title[:40]+\"...\"\n",
    "        self.guesses.append(guess)\n",
    "        self.truths.append(truth)\n",
    "        self.errors.append(error)\n",
    "        self.sles.append(sle)\n",
    "        self.colors.append(color)\n",
    "        print(f\"{COLOR_MAP[color]}{i+1}: Guess: ${guess:,.2f} Truth: ${truth:,.2f} Error: ${error:,.2f} SLE: {sle:,.2f} Item: {title}{RESET}\")\n",
    "\n",
    "    def chart(self, title):\n",
    "        max_error = max(self.errors)\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        max_val = max(max(self.truths), max(self.guesses))\n",
    "        plt.plot([0, max_val], [0, max_val], color='deepskyblue', lw=2, alpha=0.6)\n",
    "        plt.scatter(self.truths, self.guesses, s=3, c=self.colors)\n",
    "        plt.xlabel('Ground Truth')\n",
    "        plt.ylabel('Model Estimate')\n",
    "        plt.xlim(0, max_val)\n",
    "        plt.ylim(0, max_val)\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "\n",
    "    def report(self):\n",
    "        average_error = sum(self.errors) / self.size\n",
    "        rmsle = math.sqrt(sum(self.sles) / self.size)\n",
    "        hits = sum(1 for color in self.colors if color==\"green\")\n",
    "        title = f\"{self.title} Error=${average_error:,.2f} RMSLE={rmsle:,.2f} Hits={hits/self.size*100:.1f}%\"\n",
    "        self.chart(title)\n",
    "\n",
    "    def run(self):\n",
    "        self.error = 0\n",
    "        for i in range(self.size):\n",
    "            self.run_datapoint(i)\n",
    "        self.report()\n",
    "\n",
    "    @classmethod\n",
    "    def test(cls, function):\n",
    "        cls(function).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e8d122-0ccd-4bfd-b90b-7975f8ca2596",
   "metadata": {},
   "source": [
    "# Now for something basic\n",
    "\n",
    "Simplest model you could imagine - a random number generator!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "367b5680-0249-4c42-a047-7ac54752839d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_pricer(item):\n",
    "    return random.randrange(1,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e1651106-9ffd-4084-aca3-e5ec3abd7306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# Run our TestRunner\n",
    "Tester.test(random_pricer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "738d34f0-98be-4a05-be7e-440cd183b8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# That was fun!\n",
    "# We can do better - here's another rather trivial model\n",
    "\n",
    "training_prices = [item.price for item in train]\n",
    "training_average = sum(training_prices) / len(training_prices)\n",
    "\n",
    "def constant_pricer(item):\n",
    "    return training_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "55893b7f-a21c-424b-8ce0-c6a9d1431715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run our constant predictor\n",
    "Tester.test(constant_pricer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb043c1-5db1-4c63-9a30-677e39250d20",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "What properties of an item are most meaningful to predict its price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4ca6fdd2-33ce-48bf-968c-b049cb3cd576",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[0].details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2fdad4c0-7a68-4a9a-a159-f416ab9dab33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new \"features\" field on items, and populate it with json parsed from the details dict\n",
    "\n",
    "for item in train:\n",
    "    item.features = json.loads(item.details)\n",
    "for item in test:\n",
    "    item.features = json.loads(item.details)\n",
    "\n",
    "# Look at one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e2f489c1-eda6-48eb-87ba-6b9751a2798f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[0].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "491b183f-afbb-4bd3-9321-6778c6e2527d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[0].features.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2c9bd35d-69df-46e1-8336-954a7f398c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at 20 most common features in training set. Counter library will count things up - here you're counting features\n",
    "\n",
    "feature_count = Counter()\n",
    "for item in train:\n",
    "    for f in item.features.keys():\n",
    "        feature_count[f]+=1\n",
    "\n",
    "feature_count.most_common(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8c2acd61-08a7-411d-b0be-87763ab6f4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now some janky code to pluck out the Item Weight\n",
    "# Don't worry too much about this: spoiler alert, it's not going to be much use in training!\n",
    "\n",
    "def get_weight(item):\n",
    "    weight_str = item.features.get('Item Weight')\n",
    "    if weight_str:\n",
    "        parts = weight_str.split(' ')\n",
    "        amount = float(parts[0])\n",
    "        unit = parts[1].lower()\n",
    "        if unit==\"pounds\":\n",
    "            return amount\n",
    "        elif unit==\"ounces\":\n",
    "            return amount / 16\n",
    "        elif unit==\"grams\":\n",
    "            return amount / 453.592\n",
    "        elif unit==\"milligrams\":\n",
    "            return amount / 453592\n",
    "        elif unit==\"kilograms\":\n",
    "            return amount / 0.453592\n",
    "        elif unit==\"hundredths\" and parts[2].lower()==\"pounds\":\n",
    "            return amount / 100\n",
    "        else:\n",
    "            print(weight_str)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "282ac309-9fce-471c-bb5a-f9f64c71fdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [get_weight(t) for t in train]  # Get weights for all items in training data\n",
    "weights = [w for w in weights if w]       # Filters out any noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2844d661-5992-4919-94b6-8f12c40110c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_weight = sum(weights)/len(weights)\n",
    "average_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "03e297ba-70d7-47e9-9fc8-79cdfd5be42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to use average weight if we cannot find a weight \n",
    "\n",
    "def get_weight_with_default(item):\n",
    "    weight = get_weight(item)\n",
    "    return weight or average_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "427c05ca-d174-4ec8-9da5-a97efe17b535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Best Sellers Rank from its features, return a dictionary\n",
    "\n",
    "def get_rank(item):\n",
    "    rank_dict = item.features.get(\"Best Sellers Rank\")\n",
    "    if rank_dict:\n",
    "        ranks = rank_dict.values()\n",
    "        return sum(ranks)/len(ranks)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ab7d27b3-e737-4280-94d5-7be0b4eb7440",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = [get_rank(t) for t in train]\n",
    "ranks = [r for r in ranks if r]\n",
    "average_rank = sum(ranks)/len(ranks)\n",
    "average_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "51b46d1a-3ed0-404c-a846-7669a7bc3279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to use average rank from training dataset if we cannot find a rank\n",
    "\n",
    "def get_rank_with_default(item):\n",
    "    rank = get_rank(item)\n",
    "    return rank or average_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4e06aea1-07d1-4458-a57c-746cd59bd291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_length(item):\n",
    "    return len(item.test_prompt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "43fd9471-eb51-4a0f-b445-fd09e6da6794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate the brands\n",
    "\n",
    "brands = Counter()\n",
    "for t in train:\n",
    "    brand = t.features.get(\"Brand\")\n",
    "    if brand:\n",
    "        brands[brand]+=1\n",
    "\n",
    "# Look at most common 40 brands\n",
    "\n",
    "brands.most_common(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2cbb6049-3606-4ca3-b982-f94e4b67afa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_ELECTRONICS_BRANDS = [\"hp\", \"dell\", \"lenovo\", \"samsung\", \"asus\", \"sony\", \"canon\", \"apple\", \"intel\"]\n",
    "def is_top_electronics_brand(item):\n",
    "    brand = item.features.get(\"Brand\")\n",
    "    return brand and brand.lower() in TOP_ELECTRONICS_BRANDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ef76304c-97ab-4288-a843-e64ab03a5046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(item):\n",
    "    return {\n",
    "        \"weight\": get_weight_with_default(item),\n",
    "        \"rank\": get_rank_with_default(item),\n",
    "        \"text_length\": get_text_length(item),\n",
    "        \"is_top_electronics_brand\": 1 if is_top_electronics_brand(item) else 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3d736bd7-8a96-4df4-b80a-c0820cf2bad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at features in a training item\n",
    "get_features(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "34c50031-8d23-4bd2-9d17-0557f432e2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility function to convert our features into a pandas dataframe\n",
    "\n",
    "def list_to_dataframe(items):\n",
    "    features = [get_features(item) for item in items]\n",
    "    df = pd.DataFrame(features)\n",
    "    df['price'] = [item.price for item in items]\n",
    "    return df\n",
    "\n",
    "train_df = list_to_dataframe(train)\n",
    "test_df = list_to_dataframe(test[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "57237494-1f73-42db-8ac6-650c432c6151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traditional Linear Regression!\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Separate features and target\n",
    "feature_columns = ['weight', 'rank', 'text_length', 'is_top_electronics_brand']\n",
    "\n",
    "X_train = train_df[feature_columns]\n",
    "y_train = train_df['price']\n",
    "X_test = test_df[feature_columns]\n",
    "y_test = test_df['price']\n",
    "\n",
    "# Train a Linear Regression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "for feature, coef in zip(feature_columns, model.coef_):\n",
    "    print(f\"{feature}: {coef}\")\n",
    "print(f\"Intercept: {model.intercept_}\")\n",
    "\n",
    "# Predict the test set and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared Score: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ab52d153-8197-4a4e-a979-90960a7f001b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict price for a new item\n",
    "\n",
    "def linear_regression_pricer(item):\n",
    "    features = get_features(item)\n",
    "    features_df = pd.DataFrame([features])\n",
    "    return model.predict(features_df)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f1a09ef8-f727-4448-89b2-979d2fac36cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test it\n",
    "\n",
    "Tester.test(linear_regression_pricer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "91c790de-0b51-42d2-8c54-88b350269e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the next few models, we prepare our documents and prices\n",
    "# Note that we use the test prompt for the documents, otherwise we'll reveal the answer!!\n",
    "\n",
    "prices = np.array([float(item.price) for item in train])\n",
    "documents = [item.test_prompt() for item in train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "72078875-5dad-4f0d-b65c-71065636d1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the CountVectorizer for a Bag of Words model\n",
    "\n",
    "np.random.seed(42)\n",
    "vectorizer = CountVectorizer(max_features=1000, stop_words='english')  # CountVectorizer counts no. of words in a vector\n",
    "X = vectorizer.fit_transform(documents)\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X, prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "935e8daa-03aa-4c57-ad9d-5906eef2360e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of words linear regression pricer function\n",
    "\n",
    "def bow_lr_pricer(item):\n",
    "    x = vectorizer.transform([item.test_prompt()])\n",
    "    return max(regressor.predict(x)[0], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7210fb2e-e489-43fb-b735-d5f8cd10d7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test it\n",
    "\n",
    "Tester.test(bow_lr_pricer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b4863e16-c201-42b7-9eb5-8a23d6775b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The amazing word2vec model, implemented in gensim NLP library\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Preprocess the documents\n",
    "processed_docs = [simple_preprocess(doc) for doc in documents]\n",
    "\n",
    "# Train Word2Vec model\n",
    "w2v_model = Word2Vec(sentences=processed_docs, vector_size=400, window=5, min_count=1, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "116e8b55-ef87-458b-9de7-9db7e4920bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step of averaging vectors across the document is a weakness in our approach\n",
    "\n",
    "def document_vector(doc):\n",
    "    doc_words = simple_preprocess(doc)\n",
    "    word_vectors = [w2v_model.wv[word] for word in doc_words if word in w2v_model.wv]\n",
    "    return np.mean(word_vectors, axis=0) if word_vectors else np.zeros(w2v_model.vector_size)\n",
    "\n",
    "# Create feature matrix\n",
    "X_w2v = np.array([document_vector(doc) for doc in documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "05ed5f75-a8b1-47a2-a77c-8f0558ac0e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Linear Regression on word2vec\n",
    "\n",
    "word2vec_lr_regressor = LinearRegression()\n",
    "word2vec_lr_regressor.fit(X_w2v, prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "eefe107b-1873-4897-a43a-802541f403b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_lr_pricer(item):\n",
    "    doc = item.test_prompt()\n",
    "    doc_vector = document_vector(doc)\n",
    "    return max(0, word2vec_lr_regressor.predict([doc_vector])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "613096d6-829f-4442-a827-3f156cb82a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tester.test(word2vec_lr_pricer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e7fb59f9-89a0-4db4-8eec-d44fd2d6d856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "np.random.seed(42)\n",
    "svr_regressor = LinearSVR()\n",
    "\n",
    "svr_regressor.fit(X_w2v, prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7149aa33-e681-4071-b496-5648a88b13e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine model\n",
    "\n",
    "def svr_pricer(item):\n",
    "    np.random.seed(42)\n",
    "    doc = item.test_prompt()\n",
    "    doc_vector = document_vector(doc)\n",
    "    return max(float(svr_regressor.predict([doc_vector])[0]),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cfe71224-1dc2-49f3-ad45-3af89ac565d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tester.test(svr_pricer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e82be056-ec95-4cc8-8e58-073fc9e13027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And the powerful Random Forest regression\n",
    "\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=8)\n",
    "rf_model.fit(X_w2v, prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a8966171-ed93-4622-a0ad-fffe94fe1c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_pricer(item):\n",
    "    doc = item.test_prompt()\n",
    "    doc_vector = document_vector(doc)\n",
    "    return max(0, rf_model.predict([doc_vector])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cb1cbf09-06e1-4329-9d61-4295545ec78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tester.test(random_forest_pricer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
