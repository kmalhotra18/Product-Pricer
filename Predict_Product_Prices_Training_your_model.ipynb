{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPG+NWaexhuhU6r3PDNSHtY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmalhotra18/Product-Pricer/blob/main/Predict_Product_Prices_Training_your_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict Product Prices\n",
        "\n",
        "\n",
        "\n",
        "## Training your model!\n"
      ],
      "metadata": {
        "id": "rv3ZBga5v1W1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8at0z6iCvuYW"
      },
      "outputs": [],
      "source": [
        "# pip installs\n",
        "\n",
        "# trl - library from huggingface that includes SFT TRainer library to train our model\n",
        "!pip install -q datasets requests torch peft bitsandbytes transformers trl accelerate sentencepiece wandb matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, set_seed, BitsAndBytesConfig\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "import wandb\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "CF4izlEGv7eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "\n",
        "BASE_MODEL = \"meta-llama/Meta-Llama-3.1-8B\"\n",
        "PROJECT_NAME = \"pricer\"\n",
        "HF_USER = \"kmalhotra18\" # your HF name here!\n",
        "\n",
        "# Data\n",
        "\n",
        "DATASET_NAME = f\"{HF_USER}/lite-data\"\n",
        "MAX_SEQUENCE_LENGTH = 182\n",
        "\n",
        "# Run name for saving the model in the hub\n",
        "\n",
        "RUN_NAME =  f\"{datetime.now():%Y-%m-%d_%H.%M.%S}\"       # Run name is current date & time\n",
        "PROJECT_RUN_NAME = f\"{PROJECT_NAME}-{RUN_NAME}\"         # Project run name will be project run on specific date (so you can try to train with different hyperparameters etc.)\n",
        "HUB_MODEL_NAME = f\"{HF_USER}/{PROJECT_RUN_NAME}\"\n",
        "\n",
        "# Hyperparameters for QLoRA\n",
        "\n",
        "LORA_R = 8 # 32 can be on heavier box\n",
        "LORA_ALPHA = 16 # 2R\n",
        "TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
        "LORA_DROPOUT = 0.1\n",
        "QUANT_4_BIT = True\n",
        "\n",
        "# Hyperparameters for Training\n",
        "\n",
        "EPOCHS = 1 # you can do more epochs if you wish, but only 1 is needed - more is probably overkill\n",
        "BATCH_SIZE = 1 # on an A100 box this can go up to 16                # Adjustment to data point after each batch of 4/8/16/32 etc. data points (for performance mainly)\n",
        "GRADIENT_ACCUMULATION_STEPS = 1\n",
        "LEARNING_RATE = 1e-4       # Take model / training data point and do forward pass (i.e., predict next token or prob. of next possible token). Take prediction & actual to see loss, and in backward propagation do a shift in weights (i.e., learning rate)\n",
        "LR_SCHEDULER_TYPE = 'cosine'\n",
        "WARMUP_RATIO = 0.03                           # Start with lower starting rate and warm it up as you progress (to reduce initial learning rate)\n",
        "OPTIMIZER = \"paged_adamw_32bit\"               # Use optimizer to update your neural network for better outcomes\n",
        "\n",
        "# Admin config - note that SAVE_STEPS is how often it will upload to the hub\n",
        "# I've changed this from 5000 to 2000 so that you get more frequent saves\n",
        "\n",
        "STEPS = 50\n",
        "SAVE_STEPS = 2000\n",
        "LOG_TO_WANDB = True\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "htU84RcIv7bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HUB_MODEL_NAME"
      ],
      "metadata": {
        "id": "WXf32OIkv7Yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# More on Optimizers\n",
        "\n",
        "https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one#optimizer-choice\n",
        "\n",
        "The most common is Adam or AdamW (Adam with Weight Decay).  \n",
        "Adam achieves good convergence by storing the rolling average of the previous gradients; however, it adds an additional memory footprint of the order of the number of model parameters."
      ],
      "metadata": {
        "id": "3ybtQX6rwAc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Log in to HuggingFace\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "NYApjzvXv7WZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Log in to Weights & Biases\n",
        "wandb_api_key = userdata.get('WANDB_API_KEY')\n",
        "os.environ[\"WANDB_API_KEY\"] = wandb_api_key\n",
        "wandb.login()\n",
        "\n",
        "# Configure Weights & Biases to record against our project\n",
        "os.environ[\"WANDB_PROJECT\"] = PROJECT_NAME\n",
        "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\" if LOG_TO_WANDB else \"end\"\n",
        "os.environ[\"WANDB_WATCH\"] = \"gradients\""
      ],
      "metadata": {
        "id": "i3TWwGILv7Ty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(DATASET_NAME)\n",
        "train = dataset['train']\n",
        "test = dataset['test']"
      ],
      "metadata": {
        "id": "XUnEoEyYv7RI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if you wish to reduce the training dataset to 20,000 points instead, then uncomment this line:\n",
        "train = train.select(range(20000))"
      ],
      "metadata": {
        "id": "P0YstABNv7Op"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train)"
      ],
      "metadata": {
        "id": "34WyvBSQiPjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[0]"
      ],
      "metadata": {
        "id": "haeaqHVYiRQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if LOG_TO_WANDB:\n",
        "  wandb.init(project=PROJECT_NAME, name=RUN_NAME)"
      ],
      "metadata": {
        "id": "ZtxJs9Zcv7MQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now load the Tokenizer and Model\n",
        "\n",
        "The model is \"quantized\" - we are reducing the precision to 4 bits."
      ],
      "metadata": {
        "id": "kSyRp_328mRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pick the right quantization. Can also try with 8_BIT\n",
        "\n",
        "if QUANT_4_BIT:\n",
        "  quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        "  )\n",
        "else:\n",
        "  quant_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    bnb_8bit_compute_dtype=torch.bfloat16\n",
        "  )"
      ],
      "metadata": {
        "id": "g2R7qeUfv7Jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Tokenizer and the Model\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "base_model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "print(f\"Memory footprint: {base_model.get_memory_footprint() / 1e6:.1f} MB\")"
      ],
      "metadata": {
        "id": "5NKlxyeMv7G8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Collator\n",
        "\n",
        "It's important that we ensure during Training that we are not trying to train the model to predict the description of products; only their price.\n",
        "\n",
        "We need to tell the trainer that everything up to \"Price is $\" is there to give context to the model to predict the next token, but does not need to be learned.\n",
        "\n",
        "The trainer needs to teach the model to predict the token(s) after \"Price is $\".\n",
        "\n",
        "There is a complicated way to do this by setting Masks, but luckily HuggingFace provides a super simple helper class to take care of this for us."
      ],
      "metadata": {
        "id": "ZF2t3KYa8scs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import DataCollatorForCompletionOnlyLM\n",
        "response_template = \"Price is $\"\n",
        "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "a432ejRSv7EU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AND NOW\n",
        "\n",
        "## We set up the configuration for Training\n",
        "\n",
        "We need to create 2 objects:\n",
        "\n",
        "A LoraConfig object with our hyperparameters for LoRA\n",
        "\n",
        "An SFTConfig with our overall Training parameters"
      ],
      "metadata": {
        "id": "g7h4K49HA9Rr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, specify the configuration parameters for LoRA\n",
        "\n",
        "lora_parameters = LoraConfig(\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    r=LORA_R,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=TARGET_MODULES,\n",
        ")\n",
        "\n",
        "# Next, specify the general configuration parameters for training\n",
        "\n",
        "train_parameters = SFTConfig(\n",
        "    output_dir=PROJECT_RUN_NAME,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=1,\n",
        "    eval_strategy=\"no\",\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    optim=OPTIMIZER,\n",
        "    save_steps=SAVE_STEPS,\n",
        "    save_total_limit=10,\n",
        "    logging_steps=STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=0.001,\n",
        "    fp16=False,\n",
        "    bf16=True,\n",
        "    max_grad_norm=0.3,\n",
        "    max_steps=-1,\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
        "    report_to=\"wandb\" if LOG_TO_WANDB else None,\n",
        "    run_name=RUN_NAME,\n",
        "    max_seq_length=MAX_SEQUENCE_LENGTH,\n",
        "    dataset_text_field=\"text\",\n",
        "    save_strategy=\"steps\",\n",
        "    hub_strategy=\"every_save\",\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=HUB_MODEL_NAME,\n",
        "    hub_private_repo=True\n",
        ")\n",
        "\n",
        "# And now, the Supervised Fine Tuning Trainer will carry out the fine-tuning\n",
        "# Given these 2 sets of configuration parameters\n",
        "# The latest version of trl is showing a warning about labels - please ignore this warning\n",
        "\n",
        "fine_tuning = SFTTrainer(\n",
        "    model=base_model,\n",
        "    train_dataset=train,\n",
        "    peft_config=lora_parameters,\n",
        "    args=train_parameters,\n",
        "    data_collator=collator\n",
        "  )"
      ],
      "metadata": {
        "id": "0A0d5Xfwv7Bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After some time, Google might stop your colab.\n",
        "\n",
        "If your server is stopped, you can follow my colab here to resume from your last save"
      ],
      "metadata": {
        "id": "EBfHJMZeBDDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tune!\n",
        "fine_tuning.train()\n",
        "\n",
        "# Push our fine-tuned model to Hugging Face\n",
        "fine_tuning.model.push_to_hub(PROJECT_RUN_NAME, private=True)\n",
        "print(f\"Saved to the hub: {PROJECT_RUN_NAME}\")"
      ],
      "metadata": {
        "id": "nRJVI8Lrv6_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if LOG_TO_WANDB:\n",
        "  wandb.finish()"
      ],
      "metadata": {
        "id": "CoGWspVVBIF2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}